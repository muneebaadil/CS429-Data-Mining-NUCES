{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Assignment No 5c\n",
    "###### *Sibt ul Hussain*\n",
    "----\n",
    "## Goal\n",
    "\n",
    "Your goal in this assigment is to implement a Perceptron Classifier.\n",
    "\n",
    "**Note** Please note that you are allowed to use only those libraries which we have discussed in the class, i.e. numpy, scipy, pandas.\n",
    "\n",
    "## Submission Instructions\n",
    "You are required to submit the original notebook file on the Slate (with .ipynb extension), with complete set of outputs. Students failing to do so will get zero marks. \n",
    "\n",
    "*Please read each step carefully and understand it fully before proceeding with code writing*\n",
    "\n",
    "## Plagiarism\n",
    "Any form of plagiarism will not be tolerated and result in 0 marks.\n",
    "\n",
    "## For Graphical Debugging:\n",
    "You can use the [pycharm](https://www.jetbrains.com/pycharm/download/#section=linux) excellent graphical debugging based IDE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Complete the missing functions definitions in file \"perceptron.py\". You will need to write the functions hypothesis, cost_function and derivative_cost_function. **Please read the function definition before proceeding with code writing**.\n",
    "2. Complete the missing function definition gradient_descent  in file \"optimizer.py\"\n",
    "3. Run the complete notebook & check that you are getting the right results from your classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import scipy.stats\n",
    "from collections import defaultdict  # default dictionary \n",
    "plt.style.use('ggplot')\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tools as t # set of tools for plotting, data splitting, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "Given a set of $m$ $d$-dimensional labelled training examples $X$ and their labels $Y $($Y \\in \\{-1, +1\\})$.\n",
    "Your goal in this assignment is to implement perceptron classifier. Recall that a perceptron uses the hypothesis $\\begin{equation} h_\\theta(x) = x^T\\theta. \\end{equation}$ with the classification rule $sign(h_\\theta(x))$. \n",
    "\n",
    "In perceptron we try to optimize (minimize) following cost function (without regularization):\n",
    "\n",
    " $$\\begin{equation} J_\\theta = \\frac{1}{2m}\\sum_{i=1}^m  max(0,-y^{(i)} x^{(i) T}\\theta)+\\frac{\\lambda}{2}\\sum_{i=1}^k \\theta_k^2\\end{equation}$$\n",
    " \n",
    " \n",
    "Here $m$ is the number of training exmaples.\n",
    "\n",
    "\n",
    "We will be adding an extra column to our X input matrix for the offset, then we can write our hypothesis in the form of matrix-vector product. I.e. earlier we were writing our hypothesis as: $h_\\theta(x^i)=\\theta_0+ x^i *\\theta_1$, [*Remember the notation we are using, superscript is being used to represent the example, and subscript is being used for representing the feature, so $x^i_j$ means j-th feature of i-th example in our set*]\n",
    "\n",
    "Since we can write this expression in the form of dot product, i.e.  $h_\\theta(x^i)=x^{(i)T}\\theta$\n",
    "\n",
    "So to simplify the calculations we will append an extra 1 at the start of each example to perform these computations using matrix-vector product.\n",
    " \n",
    "Recall the partial derivative of the cost function wrt $\\theta_j$ for a single example will be =\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_j}= \\lambda * \\theta_j+ \\begin{cases}-y\\cdot x_j & \\text{if $y\\cdot x^T \\theta <0$}, \\\\ 0 &\n",
    "\\text{otherwise}.\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from perceptron import * \n",
    "from preprocessing import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create some dummy data for testing\n",
    "Please read the code carefully and see whats it is doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some dummy data for testing\n",
    "\n",
    "np.random.seed(seed=99)\n",
    "\n",
    "# make some data up\n",
    "mean1 = [-3,-3]\n",
    "mean2 = [2,2]\n",
    "cov = [[1.0,0.0],[0.0,1.0]] \n",
    "\n",
    "#create some points\n",
    "nexamples=500\n",
    "x1 = np.random.multivariate_normal(mean1,cov,nexamples/2)\n",
    "x2 = np.random.multivariate_normal(mean2,cov,nexamples/2)\n",
    "\n",
    "X=np.vstack((x1,x2))\n",
    "Y=np.vstack((1*np.ones((nexamples/2,1)),-1*np.ones((nexamples/2,1))))\n",
    "\n",
    "plt.scatter(x1[:,0],x1[:,1], c='r', s=100)\n",
    "plt.scatter(x2[:,0],x2[:,1], c='b', s=100)\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"Linear Classification\")\n",
    "plt.xlabel(\"feature $x_1$\")\n",
    "plt.ylabel(\"feature $x_2$\")\n",
    "\n",
    "fig_ml_in_10 = plt.gcf()\n",
    "plt.savefig('linear-class-percep.svg',format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape,Y.shape, max(Y),min(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scale the features....\n",
    "preprocess=PreProcessing(X)\n",
    "X=preprocess.process_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets append a vector of dummy 1's at the start of X to simplify the calculations...\n",
    "X=np.hstack((X,np.ones((X.shape[0],1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Classifier Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#create a perceptron class object\n",
    "percep=Perceptron(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Check the Derivatives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets check the derivatives of perceptron, \n",
    "\n",
    "#Please note that these derivatives of perceptron can fluctuate, due to kink at zero \n",
    "#right way of checking it we after derivative, cost function value must be zero..;\n",
    "from optimizer import *\n",
    "Optimizer.gradient_check(X,Y,percep.cost_function,percep.derivative_cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percep.train(X,Y,Optimizer(alpha=0.01)) # your cost function at the minimum must be zero..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets plot the decision boundary...\n",
    "plt.scatter(x1[:,0],x1[:,1], c='r', s=100)\n",
    "plt.scatter(x2[:,0],x2[:,1], c='b', s=100)\n",
    "\n",
    "minx=min(X[:,0])\n",
    "maxx=max(X[:,0])\n",
    "\n",
    "y1=(-percep.theta[2]-percep.theta[0]*minx)/percep.theta[1]\n",
    "y2=(-percep.theta[2]-percep.theta[0]*maxx)/percep.theta[1]\n",
    "print y1, y2\n",
    "plt.plot([minx,y1],[maxx,y2], c='g', linewidth=5.0)\n",
    "\n",
    "plt.title(\"Linear Classification\")\n",
    "plt.xlabel(\"feature $x_1$\")\n",
    "plt.ylabel(\"feature $x_2$\")\n",
    "\n",
    "fig_ml_in_10 = plt.gcf()\n",
    "plt.savefig('linear-class-temp.svg',format='svg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npts=10000\n",
    "model=percep\n",
    "ax=plt.gca()\n",
    "x0spr = max(X[:,0])-min(X[:,0])\n",
    "x1spr = max(X[:,1])-min(X[:,1])\n",
    "\n",
    "tx=np.random.rand(npts,2)\n",
    "tx[:,0] = tx[:,0]*x0spr + min(X[:,0])\n",
    "tx[:,1] = tx[:,1]*x1spr + min(X[:,1])\n",
    "\n",
    "tx=np.hstack((tx,np.ones((tx.shape[0],1))))\n",
    "print tx.shape\n",
    "cs= model.predict(tx)\n",
    "print cs, np.unique(cs)\n",
    "ax.scatter(tx[:,0],tx[:,1],c=cs, alpha=.35)\n",
    "\n",
    "ax.hold(True)\n",
    "ax.scatter(X[:,0],X[:,1],\n",
    "             c=list(map(lambda x:'r' if x==1 else 'lime',Y)), \n",
    "             linewidth=0,s=25,alpha=1)\n",
    "ax.set_xlim([min(X[:,0]), max(X[:,0])])\n",
    "ax.set_ylim([min(X[:,1]), max(X[:,1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the data set\n",
    "data=pd.read_csv('./iris.data')\n",
    "data.columns=['SepalLength','SepalWidth','PetalLength','PetalWidth','Class']\n",
    "print data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get your data in matrix\n",
    "X=np.asarray(data[['SepalLength','SepalWidth','PetalLength','PetalWidth']].dropna())\n",
    "Y=np.asarray(data['Class'].dropna())\n",
    "print \" Data Set Dimensions=\", X.shape, \" True Class labels dimensions\", Y.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess=PreProcessing(X)\n",
    "X=preprocess.process_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y[Y=='Iris-virginica']='Iris-versicolor'\n",
    "print Y, len(Y), np.unique(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y[Y=='Iris-versicolor']=-1\n",
    "Y[Y=='Iris-setosa']=+1\n",
    "#Lets append a vector of dummy 1's at the start of X to simplify the calculations...\n",
    "X=np.hstack((X,np.ones((X.shape[0],1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percep=Perceptron(lembda=0.00)\n",
    "feat=[0,1,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see the documentation of split_data in tools for further information...\n",
    "Xtrain,Ytrain,Xtest,Ytest=t.split_data(X,Y)\n",
    "Ytrain=Ytrain.reshape(len(Ytrain),1)\n",
    "Ytest=Ytest.reshape(len(Ytest),1)\n",
    "print \" Training Data Set Dimensions=\", Xtrain.shape, \"Training True Class labels dimensions\", Ytrain.shape   \n",
    "print \" Test Data Set Dimensions=\", Xtest.shape, \"Test True Class labels dimensions\", Ytest.shape   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "percep.train(Xtrain,Ytrain,Optimizer(alpha=0.0001)) # your cost function at the minimum must be zero..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets test it on the set of unseen examples...\n",
    "pclasses=percep.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets see how good we are doing, by finding the accuracy on the test set..\n",
    "print np.sum(pclasses==Ytest)\n",
    "print \"Accuracy = \", np.sum(pclasses==Ytest)/float(Ytest.shape[0])\n",
    "t.print_confusion_matrix(pclasses.ravel(),Ytest.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "41adc71caba60e4b5fef12c8930b203a",
     "grade": true,
     "grade_id": "test_acc",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_greater_equal\n",
    "acc = np.sum(pclasses==Ytest)/float(Ytest.shape[0])\n",
    "assert_greater_equal(acc, 0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
